import Image from "next/image";
import {
  Carousel,
  CarouselContent,
  CarouselItem,
  CarouselNext,
  CarouselPrevious,
} from "@/components/ui/carousel";
import { Card, CardContent } from "@/components/ui/card";
import { PlayIcon, ArrowTopRightIcon } from "@radix-ui/react-icons";


export default function Home() {
  return (
    <div className="">
      <main className="flex flex-col gap-8  p-8 pb-20 gap-16 sm:p-20 md:px-48 ">
        <div className="flex flex-col items-center gap-8">
        <h3>November 1, 2024</h3>
        <h1 className="text-5xl font-bold text-center">Oasis: Generating Worlds in Realtime</h1>
        <h2 className="mt-[-20px] text-center mb-8"><a className="underline" href="https://etched.ai">Decart Team</a>, <a className="underline" href="https://decart.ai">Etched Team</a></h2>
        <video src="/wide1.mp4" autoPlay loop muted playsInline className="w-full h-full object-cover" />

        <div className="flex flex-col items-center justify-center gap-4 w-full md:w-1/2">
        <a
            className="rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-14 sm:h-16 px-4 sm:px-5 w-56"
            href=""
            target="_blank"
            rel="noopener noreferrer"
          >
            <PlayIcon className="w-4 h-4" />
            <b>
            Play demo
            </b>
          </a>
          </div>
        <div className="flex gap-4 flex-col sm:flex-row w-full items-center justify-center mt-[-14px]">

        <a
            className="rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5"
            href=""
            target="_blank"
            rel="noopener noreferrer"
          >
            <ArrowTopRightIcon className="w-4 h-4" />
            Decart Blog
          </a>
          <a
            className="rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5"
            href=""
            target="_blank"
            rel="noopener noreferrer"
          >
            <Image
              className="dark:invert"
              src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg"
              alt="GitHub"
              width={20}
              height={20}
            />
            View code
          </a>

          <a
            className="rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5"
            href=""
            target="_blank"
            rel="noopener noreferrer"
          >
            <ArrowTopRightIcon className="w-4 h-4" />
            Etched Blog
          </a>
        </div>
        </div>
        <div className="flex flex-col gap-4 px-0 lg:px-32">
        <p className="text-justify leading-relaxed">We&apos;re excited to announce Oasis, the first realtime AI world model. It&apos;s a video game, but entirely generated by AI. Oasis is the first step in our research towards more complex interactive worlds. We&apos;re releasing the code, open-source weights and a live playable demo.</p>

        <p className="text-justify leading-relaxed">Oasis takes keyboard input and generates open-world gameplay in real-time - including physics, game rules and graphics, which it learned purely from watching tens of thousands of hours of recorded clips. You can move around, jump and break blocks like in the original game.</p>

        <div className="relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4">
          <Carousel className="w-full">
            <CarouselContent>
              {["/5.webp", "/4.webp", "/1.webp", "/2.webp", "/3.webp"].map((src, index) => (
                <CarouselItem key={index} className="basis-1/2 md:basis-1/3">
                  <Card>
                    <CardContent className="flex items-center justify-center p-0">
                      <Image
                        src={src}
                        alt={`Carousel image ${index + 1}`}
                        width={700}
                        height={300}
                        className="object-cover"
                      />
                    </CardContent>
                  </Card>
                </CarouselItem>
              ))}
            </CarouselContent>
            <div className="flex justify-center gap-2 mt-4">
              <CarouselPrevious className="static translate-y-0" />
              <CarouselNext className="static translate-y-0" />
            </div>
          </Carousel>
        </div>
          

        <p className="text-justify leading-relaxed">Our model even understands complex game mechanics, such as crafting, and NPCs.</p>

        <div className="relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4">
          <Carousel className="w-full">
            <CarouselContent>
              {["/1.webp", "/2.webp", "/3.webp", "/4.webp", "/5.webp"].map((src, index) => (
                <CarouselItem key={index} className="basis-1/2 md:basis-1/3">
                  <Card>
                    <CardContent className="flex items-center justify-center p-0">
                      <Image
                        src={src}
                        alt={`Carousel image ${index + 1}`}
                        width={700}
                        height={300}
                        className="object-cover"
                      />
                    </CardContent>
                  </Card>
                </CarouselItem>
              ))}
            </CarouselContent>
            <div className="flex justify-center gap-2 mt-4">
              <CarouselPrevious className="static translate-y-0" />
              <CarouselNext className="static translate-y-0" />
            </div>
          </Carousel>
        </div>
          
        <h2 className="text-2xl font-bold text-center mt-8">Architecture</h2>

        <p className="text-justify leading-relaxed">The model is composed of two parts - a spatial autoencoder, and a latent diffusion backbone. Both are Transformer-based - the autoencoder is based on ViT<sup className="text-gray-500">[1]</sup>, and the backbone is based on DiT<sup className="text-gray-500">[2]</sup>. We chose Transformers to ensure stable, predictable scaling, and fast inference on Etched&apos;s Transformer ASIC chip, Sohu.</p>

        <Image src="/arch_new.png" alt="Architecture" width={1000} height={500} />

        <p className="text-justify leading-relaxed">In contrast to bidirectional models such as Sora<sup className="text-gray-500">[3]</sup>, Oasis can generate an unlimited length of video auto-regressively, with the option for each frame to be conditioned on game input. This enables users to interact with the world in realtime. The model was trained using Diffusion Forcing<sup className="text-gray-500">[4]</sup>, which denoises with independent per-token noise levels, and allows for novel decoding schemes such as ours.</p>

        <Image src="/difforce.png" alt="Diffusion Forcing" width={1000} height={500} />

        <p className="text-justify leading-relaxed">Because we generate frames auto-regressively, we can use causal attention between frames rather than full bidirectional attention, as seen in other video models. This allows us to generate theoretically infinite sequences, and run the model much faster than otherwise possible. Causal attention allows us to take advantage of several LLM inference optimization strategies, such as KV caching<sup className="text-gray-500">[5]</sup>.</p>

        <p className="text-justify leading-relaxed">One issue we focused on is temporal stability, or making sure the model outputs make sense over long time horizons. In auto-regressive models, errors compound, and small imperfections can quickly snowball into glitched frames. Solving this required innovations in long-context generation.</p>
        <p className="text-justify leading-relaxed">A strategy we found to greatly increase stability was dynamic noising, which adjusts noise at inference time on a schedule, injecting noise in the first diffusion forward passes to reduce error accumulation, but gradually removing noise in the later passes so the model can find and persist high-frequency details in previous frames for improved consistency. Since our model saw noise during training, it was able to successfully deal with noisy samples at inference.</p>

        <div className="flex justify-center items-center">
        <Image src="/dyno.png" alt="Dynamic Noising" width={700} height={500} />
        </div>

        <p className="text-justify leading-relaxed">Training this new model posed giant engineering and research challenges, which the Decart team tackled head-on. Key to our success was tuning our training recipeâ€”optimizing batch sizes, hyperparameters, learning rate schedules, and architectures. On the systems side, we leveraged distributed storage to clear data bottlenecks, accelerated NCCL communications, and fused CUDA kernels, resulting in 10x faster training, enabling rapid model iterations.</p>
        <p className="text-justify leading-relaxed">To learn more about the engineering underlying this model, and some of the specific optimizations in training and inference, check out the <a className="underline" href="https://decart.ai/blog/training-the-world-model">Decart blog post</a>.</p>

        <h2 className="text-2xl font-bold text-center mt-8">Scaling</h2>

        <p className="text-justify leading-relaxed">We train two model sizes - Oasis-500M and Oasis-1B. Both models output similar aesthetic quality. We find that the 1B model is more stable and learns some more complex behaviour, such as inventory management and temporal consistency. [Real comparsion clips pending]</p>
        <div className="relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4">
          <Carousel className="w-full">
            <CarouselContent>
              {["/5.webp", "/4.webp", "/1.webp", "/2.webp", "/3.webp"].map((src, index) => (
                <CarouselItem key={index} className="basis-1/2 md:basis-1/3">
                  <Card>
                    <CardContent className="flex items-center justify-center p-0">
                      <Image
                        src={src}
                        alt={`Carousel image ${index + 1}`}
                        width={700}
                        height={300}
                        className="object-cover"
                      />
                    </CardContent>
                  </Card>
                </CarouselItem>
              ))}
            </CarouselContent>
            <div className="flex justify-center gap-2 mt-4">
              <CarouselPrevious className="static translate-y-0" />
              <CarouselNext className="static translate-y-0" />
            </div>
          </Carousel>
        </div>
          
        <h2 className="text-2xl font-bold text-center mt-8">Performance</h2>

        <p className="text-justify leading-relaxed">Oasis is able to generate output in realtime, which means 24 frames every second. Current state-of-the-art text-to-video models with a similar DiT architecture (e.g. Sora<sup className="text-gray-500">[3]</sup>, Mochi-1<sup className="text-gray-500">[6]</sup> and Runway<sup className="text-gray-500">[7]</sup>) can take 10-20 seconds to create just one second of video, even on multiple GPUs. In order to match the experience of playing a game, however, our model must generate a new frame every 0.04 seconds, which is over 100x faster.</p>

        <section className="flex justify-center items-center my-4">
        <Image src="/speed.png" alt="Performance" width={500} height={500} />
        </section>

        <p className="text-justify leading-relaxed">Part of the optimization effort to achieve this was in software - Decart has pioneered a new AI inference stack written from scratch in CUDA that far outpaces any open-source equivalent. With Decart&apos;s optimizations, the model was able to run at playable framerates, unlocking real-time interactivity for the first time. Read more about it on <a className="underline" href="https://decart.ai/blog/training-the-world-model">Decart&apos;s blog</a>.</p>

        <p className="text-justify leading-relaxed">However, to make the model an additional order of magnitude faster, and make it cost-efficient to run at scale, new hardware is needed. Oasis is optimized for Sohu, the Transformer ASIC built by Etched. On NVIDIA H100s today, the model can run at 720p at 20fps. Sohu can run the same model at up to 4K, which is 4x more tokens per second. </p>

        <p className="text-justify leading-relaxed">In addition, Oasis&apos; end-to-end Transformer architecture makes it extremely efficient on Sohu - at the same price and power consumption as an H100 GPU, Oasis on Sohu can serve 10x more users. We believe the price of serving models like Oasis is the hidden bottleneck to releasing generative video in production. See more performance figures and read more about Oasis and Sohu on <a className="underline" href="https://etched.ai/blog">Etched&apos;s blog</a>.</p>

        <section className="flex justify-center items-center my-4">
        <Image src="/sohu.png" alt="Performance" width={500} height={500} />
        </section>

        <h2 className="text-2xl font-bold text-center mt-8">Future Work</h2>

        <p className="text-justify leading-relaxed">Oasis is an impressive technical demo, but we believe this research will enable an exciting new generation of foundation models and consumer products. For example:</p>
        <ul className="list-disc list-inside">
          <li>Creating and editing game content on-the-fly, even while playing, through text and image prompting</li>
          <li>Generating content individually tailored for each user on social media</li>
          <li>Real-time medical care, able to respond on the fly to patients in a video call</li>
          <li>AI teachers, who can generate videos to respond to students in a classroom</li>
        </ul>
        <p className="text-justify leading-relaxed">Oasis is the first in a series of world generation models - we&apos;re scaling our dataset and architecture by 10x each, and we&apos;re excited to report our findings soon.</p>
        <p className="text-justify leading-relaxed">Etched and Decart are excited to build these models together. The integration of Oasis with our hardware and software, from architecture to production, will ensure this model family remains one of the fastest and best as we advance the frontier of world generation. [If you&apos;re interested in partnering, reach out to @decart.ai]</p>
      </div>

      


        </main>

        <div className="flex flex-col gap-4 p-8 sm:p-20 md:px-48 bg-secondary">
          <p className="text-justify text-sm text-secondary-foreground">[1]: <a className="underline" href="https://arxiv.org/abs/2010.11929">Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p>
          <p className="text-justify text-sm text-secondary-foreground">[2]: <a className="underline" href="https://arxiv.org/abs/2212.09748">Peebles et al., Scalable Diffusion Models with Transformers</a></p>
          <p className="text-justify text-sm text-secondary-foreground">[3]: <a className="underline" href="https://openai.com/index/video-generation-models-as-world-simulators/">OpenAI, Video generation models as world simulators</a></p>
          <p className="text-justify text-sm text-secondary-foreground">[4]: <a className="underline" href="https://arxiv.org/abs/2407.01392">Chen et al., Diffusion Forcing: Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</a></p>
          <p className="text-justify text-sm text-secondary-foreground">[5]: <a className="underline" href="https://arxiv.org/abs/2211.05102">Pope et al., Efficiently Scaling Transformer Inference</a></p>
          <p className="text-justify text-sm text-secondary-foreground">[6]: <a className="underline" href="https://www.genmo.ai/blog">Genmo, Mochi 1: A new SOTA in open-source video generation models</a></p>
          <p className="text-justify text-sm text-secondary-foreground">[7]: <a className="underline" href="https://runwayml.com/research/introducing-gen-3-alpha">Runway, Introducing Gen-3 Alpha: A New Frontier for Video Generation</a></p>
          <p className="text-justify text-sm text-secondary-foreground">* Estimated throughput figures - Sora reported, Mochi-1 from FAL.AI endpoint adjusted for parameter count, Runway from Gen-3 reported throughput</p>
          </div>

      <div className="flex flex-col gap-4 px-0 md:px-16 bg-black">
        <div className="flex flex-col gap-4 p-8 pb-20 gap-16 sm:p-20 md:px-48">
              <h2 className="text-2xl font-bold text-center text-white">Contributors</h2>
              <p className="text-white text-center">Decart Team</p>
              <p className="text-white text-center">Etched: <a className="underline" href="">Julian Quevedo</a>, <a className="underline" href="">Quinn McIntyre</a>, <a className="underline" href="">Spruce Campbell</a>, <a className="underline" href="">Robert Wachen</a></p>
              
        </div>
        </div>

    

      
    </div>
  );
}
